{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqrp/F0xEEzeASyyhAjZTW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XoD8tGQuixw7"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import patches\n","from collections import Counter\n","import cv2\n","from glob import glob\n","from tqdm import tqdm\n","from termcolor import colored\n","\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","from torchvision import transforms\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"markdown","source":["1. Defind Dataset"],"metadata":{"id":"XfVPzX3NlXlv"}},{"cell_type":"code","source":["class CustomVOCDataset(torchvision.datasets.VOCDetection):\n","    def __init__(self, root, year=\"2012\", image_set=\"train\", download=False,\n","                 class_mapping=None, S=7, B=2, C=20, custom_transforms=None):\n","        # Initialize YOLO-specific configuration parameters.\n","        super(CustomVOCDataset, self).__init__(root, year, image_set, download)\n","        self.S = S  # Grid size\n","        self.B = B  # Number of bounding boxes\n","        self.C = C  # Number of classes\n","        self.class_mapping = class_mapping  # Mapping of class names to class indices\n","        self.custom_transforms = custom_transforms\n","\n","    def __getitem__(self, index):\n","        # Get an image and its target (annotations) from the VOC dataset.\n","        image, target = super(CustomVOCDataset, self).__getitem__(index)\n","        img_width, img_height = image.size\n","\n","        # Convert target annotations to YOLO format\n","        boxes = convert_to_yolo_format(target, img_width, img_height, self.class_mapping)\n","\n","        # Separate box coordinates and labels\n","        just_boxes = boxes[:, 1:]\n","        labels = boxes[:, 0]\n","\n","        # Apply custom transformations\n","        if self.custom_transforms:\n","            sample = {\n","                'image': np.array(image),\n","                'bboxes': just_boxes,\n","                'labels': labels\n","            }\n","            sample = self.custom_transforms(**sample)\n","            image = sample['image']\n","            just_boxes = sample['bboxes']\n","            labels = sample['labels']\n","\n","        # Create an empty label matrix for YOLO ground truth\n","        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n","\n","        # Convert to PyTorch tensors\n","        boxes = torch.tensor(just_boxes, dtype=torch.float32)\n","        labels = torch.tensor(labels, dtype=torch.float32)\n","        image = torch.as_tensor(image, dtype=torch.float32)\n","\n","        # Iterate through each bounding box in YOLO format .\n","        for box, class_label in zip(boxes, labels):\n","            x, y, width, height = box.tolist()\n","            class_label = int(class_label)\n","\n","            # Calculate the grid cell (i, j) that this box belongs to.\n","            i, j = int(self.S * y), int(self.S * x)\n","            x_cell, y_cell = self.S * x - j, self.S * y - i\n","\n","            # Calculate the width and height of the box relative to the grid cell\n","            width_cell, height_cell = width * self.S, height * self.S\n","\n","            # Check if this grid cell already has an object\n","            if label_matrix[i, j, 20] == 0:\n","                # Mark that an object exists in this cell .\n","                label_matrix[i, j, 20] = 1  # Object exists\n","\n","                # Store the box coordinates as an offset from the cell boundaries\n","                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n","\n","                # Set the box coordinates in the label matrix .\n","                label_matrix[i, j, 21:25] = box_coordinates\n","\n","                # Set the one-hot encoding for the class label .\n","                label_matrix[i, j, class_label] = 1\n","\n","        return image, label_matrix"],"metadata":{"id":"2JmCPwBOkGyc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Change Annotation to YOLO format"],"metadata":{"id":"0cNwdecHlWMT"}},{"cell_type":"code","source":["def convert_to_yolo_format ( target , img_width , img_height , class_mapping ) :\n","    \"\"\"\n","    Convert annotation data from VOC format to YOLO format .\n","\n","    Parameters :\n","        target ( dict ): Annotation data from VOCDetection dataset .\n","        img_width ( int): Width of the original image .\n","        img_height ( int): Height of the original image .\n","        class_mapping ( dict ): Mapping from class names to integer IDs.\n","    Returns :\n","        torch.Tensor: Tensor of shape [N, 5] for N bounding boxes,\n","                    each with [ class_id , x_center , y_center , width , height ].\n","    \"\"\"\n","    # Extract the list of annotations from the target dictionary.\n","    annotations = target['annotation']['object']\n","\n","    # Get the real width and height of the image from the annotation.\n","    real_width = int(target['annotation']['size']['width'])\n","    real_height = int(target['annotation']['size']['height'])\n","\n","    # Ensure annotations is a list, even if there's only one object.\n","    if not isinstance(annotations, list):\n","        annotations = [annotations]\n","\n","    # Initialize an empty list to store the converted bounding boxes.\n","    boxes = []\n","\n","    # Loop through each annotation and convert it to YOLO format.\n","    for anno in annotations:\n","        # Normalize bounding box coordinates.\n","        xmin = int(anno['bndbox']['xmin']) / real_width\n","        xmax = int(anno['bndbox']['xmax']) / real_width\n","        ymin = int(anno['bndbox']['ymin']) / real_height\n","        ymax = int(anno['bndbox']['ymax']) / real_height\n","\n","        # Calculate the center coordinates, width, and height of the bounding box.\n","        x_center = (xmin + xmax) / 2\n","        y_center = (ymin + ymax) / 2\n","        width = xmax - xmin\n","        height = ymax - ymin\n","\n","        # Retrieve the class name from the annotation and map it to an integer ID.\n","        class_name = anno['name']\n","        class_id = class_mapping.get(class_name, -1)  # Default to -1 if class not found\n","\n","        # Append the YOLO formatted bounding box to the list.\n","        boxes.append([class_id, x_center, y_center, width, height])\n","\n","    # Convert the list of boxes to a NumPy array.\n","    return np.array(boxes)"],"metadata":{"id":"UQRyEEm8leAa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Defind IoU func and NMS"],"metadata":{"id":"1AE95YcYl9sX"}},{"cell_type":"code","source":["def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n","    \"\"\"\n","    Calculate the Intersection over Union (IoU) between bounding boxes.\n","\n","    Parameters:\n","        boxes_preds (tensor): Predicted bounding boxes (BATCH_SIZE, 4).\n","        boxes_labels (tensor): Ground truth bounding boxes (BATCH_SIZE, 4).\n","        box_format (str): Box format, can be \"midpoint\" or \"corners\".\n","\n","    Returns:\n","        tensor: Intersection over Union scores for each example.\n","    \"\"\"\n","    if box_format == \"midpoint\":\n","        # Calculate coordinates of top - left (x1 , y1)\n","        # and bottom - right (x2 , y2) points for predicted boxes\n","\n","        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n","        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n","        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n","        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n","\n","        # Calculate coordinates of top - left (x1 , y1)\n","        # and bottom - right (x2 , y2) points for ground truth boxes\n","        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n","        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n","        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n","        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n","\n","    elif box_format == \"corners\":\n","        # Extract coordinates for predicted boxes\n","        box1_x1 = boxes_preds[..., 0:1]\n","        box1_y1 = boxes_preds[..., 1:2]\n","        box1_x2 = boxes_preds[..., 2:3]\n","        box1_y2 = boxes_preds[..., 3:4]\n","\n","        # Extract coordinates for ground truth boxes\n","        box2_x1 = boxes_labels[..., 0:1]\n","        box2_y1 = boxes_labels[..., 1:2]\n","        box2_x2 = boxes_labels[..., 2:3]\n","        box2_y2 = boxes_labels[..., 3:4]\n","\n","    else:\n","        raise ValueError(\"box_format must be either 'midpoint' or 'corners'.\")\n","\n","    # Calculate intersection coordinates\n","    x1 = torch.max(box1_x1, box2_x1)\n","    y1 = torch.max(box1_y1, box2_y1)\n","    x2 = torch.min(box1_x2, box2_x2)\n","    y2 = torch.min(box1_y2, box2_y2)\n","\n","    # Compute intersection area\n","    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n","\n","    # Compute areas of both boxes (pred & groundtruth)\n","    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n","    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n","\n","    # Compute IoU\n","    iou = intersection / (box1_area + box2_area - intersection + 1e-6)\n","\n","    return iou"],"metadata":{"id":"L3CVvm4zmAdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n","    \"\"\"\n","    Perform Non-Maximum Suppression (NMS) on a list of bounding boxes.\n","\n","    Parameters:\n","        bboxes (list): List of bounding boxes, each represented as\n","                       [class_pred, prob_score, x1, y1, x2, y2].\n","        iou_threshold (float): IoU threshold to determine correct predicted bboxes.\n","        threshold (float): threshold to discard low-confidence boxes.\n","        box_format (str): \"midpoint\" or \"corners\" to specify the format of bounding boxes.\n","\n","    Returns:\n","        list: List of bounding boxes after performing NMS.\n","    \"\"\"\n","    # Ensure the input is a list\n","    assert isinstance(bboxes, list), \"bboxes must be a list.\"\n","\n","    # Filter boxes based on the probability threshold\n","    bboxes = [box for box in bboxes if box[1] > threshold]\n","\n","    # Sort boxes by probability score in descending order\n","    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n","\n","    bboxes_after_nms = []\n","\n","    # Continue looping until the list of bounding boxes is empty\n","    while bboxes:\n","        # get the box with the highest probability\n","        chosen_box = bboxes.pop(0)\n","\n","        # remove bboxes with IoU greater than the threshold with the chosen box\n","        bboxes = [\n","            box for box in bboxes\n","            if box[0] != chosen_box[0] or\n","               intersection_over_union(\n","                   torch.tensor(chosen_box[2:]),\n","                   torch.tensor(box[2:]),\n","                   box_format=box_format\n","               ) < iou_threshold\n","        ]\n","\n","        # Append the chosen box to the result list\n","        bboxes_after_nms.append(chosen_box)\n","\n","    # Return the list of bounding boxes after NMS\n","    return bboxes_after_nms"],"metadata":{"id":"Bsy2drE1nEKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mean_average_precision(\n","    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n","):\n","    \"\"\"\n","    Calculate the mean average precision (mAP).\n","\n","    Parameters:\n","        pred_boxes (list): Predicted bounding boxes, each defined as\n","            [train_idx, class_pred, prob_score, x1, y1, x2, y2].\n","        true_boxes (list): Ground truth bounding boxes, similar format to pred_boxes.\n","        iou_threshold (float): IoU threshold for considering a prediction correct.\n","        box_format (str): \"midpoint\" or \"corners\" for bounding box format.\n","        num_classes (int): Number of classes.\n","\n","    Returns:\n","        float: The mean average precision (mAP) value across all classes.\n","    \"\"\"\n","    # List to store the average precision for each class\n","    average_precisions = []\n","    epsilon = 1e-6  # Small value to prevent division by zero\n","\n","    for c in range(num_classes):\n","        # Get predictions and ground truths for the current class\n","        detections = [d for d in pred_boxes if d[1] == c]\n","        ground_truths = [gt for gt in true_boxes if gt[1] == c]\n","\n","        # Count the number of ground truth boxes for each image\n","        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","        for key, val in amount_bboxes.items():\n","            amount_bboxes[key] = torch.zeros(val)\n","\n","        # Sort detections by confidence score in descending order\n","        detections.sort(key=lambda x: x[2], reverse=True)\n","\n","        # Initialize true positives (TP) and false positives (FP)\n","        TP = torch.zeros(len(detections))\n","        FP = torch.zeros(len(detections))\n","\n","        # Total ground truth boxes for the current class\n","        total_true_bboxes = len(ground_truths)\n","\n","        # Skip class if no ground truth boxes exist\n","        if total_true_bboxes == 0:\n","            continue\n","\n","        # Process each detection\n","        for detection_idx, detection in enumerate(detections):\n","            # Get ground truths for the same image\n","            ground_truth_img = [gt for gt in ground_truths if gt[0] == detection[0]]\n","\n","            best_iou = 0\n","            best_gt_idx = -1\n","\n","            # Calculate IoU for the detection with all ground truth boxes in the image\n","            for idx, gt in enumerate(ground_truth_img):\n","                iou = intersection_over_union(\n","                    torch.tensor(detection[3:]),\n","                    torch.tensor(gt[3:]),\n","                    box_format=box_format,\n","                )\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_gt_idx = idx\n","\n","            # Determine if the detection is a true positive or false positive\n","            if best_iou > iou_threshold:\n","                # Mark the ground truth box as matched\n","                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","                    TP[detection_idx] = 1\n","                    amount_bboxes[detection[0]][best_gt_idx] = 1\n","                else:\n","                    FP[detection_idx] = 1\n","            else:\n","                FP[detection_idx] = 1\n","\n","        # Cumulative sums of TP and FP\n","        TP_cumsum = torch.cumsum(TP, dim=0)\n","        FP_cumsum = torch.cumsum(FP, dim=0)\n","\n","        # Calculate recall and precision\n","        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n","        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n","\n","        # Add starting points for precision-recall curve\n","        precisions = torch.cat((torch.tensor([1]), precisions))\n","        recalls = torch.cat((torch.tensor([0]), recalls))\n","\n","        # Compute average precision using the area under the precision-recall curve\n","        average_precisions.append(torch.trapz(precisions, recalls))\n","\n","    # Return the mean of the average precisions across all classes\n","    return sum(average_precisions) / len(average_precisions)"],"metadata":{"id":"L-kcNfPpoaPk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Defind YOLOv1 architecture"],"metadata":{"id":"o1hqzBOdof-h"}},{"cell_type":"code","source":["\"\"\"\n","Information about the architectural configuration:\n","- Each tuple represents (kernel_size, number_of_filters, stride, padding).\n","- \"M\" indicates a max-pooling layer with a 2x2 pool size and stride.\n","- Nested lists represent repeated blocks, where the third element specifies the number of repetitions.\n","\"\"\"\n","\n","# Architecture configuration for YOLOv1\n","architecture_config = [\n","    (7, 64, 2, 3),              # Conv Block 1\n","    \"M\",                        # Max Pooling\n","    (3, 192, 1, 1),             # Conv Block 2\n","    \"M\",                        # Max Pooling\n","    (1, 128, 1, 0),             # Conv Block 3\n","    (3, 256, 1, 1),             # Conv Block 4\n","    (1, 256, 1, 0),             # Conv Block 5\n","    (3, 512, 1, 1),             # Conv Block 6\n","    \"M\",                        # Max Pooling\n","    [(1, 256, 1, 0), (3, 512, 1, 1), 4],  # Conv Block 7 (repeated 4 times)\n","    (1, 512, 1, 0),             # Conv Block 8\n","    (3, 1024, 1, 1),            # Conv Block 9\n","    \"M\",                        # Max Pooling\n","    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],  # Conv Block 10 (repeated 2 times)\n","    (3, 1024, 1, 1),            # Conv Block 11\n","    (3, 1024, 2, 1),            # Conv Block 12\n","    (3, 1024, 1, 1),            # Conv Block 13\n","    (3, 1024, 1, 1),            # Conv Block 14\n","]"],"metadata":{"id":"QAclL37Hoj_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNNBlock(nn.Module):\n","    \"\"\"\n","    A convolutional block consisting of:\n","    - Conv2d layer\n","    - BatchNorm2d\n","    - LeakyReLU activation\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super(CNNBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))"],"metadata":{"id":"HOLdkyPtonEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Yolov1(nn.Module):\n","    \"\"\"\n","    YOLOv1 Model:\n","    - Combines convolutional layers for feature extraction and fully connected layers for bounding box prediction.\n","    \"\"\"\n","    def __init__(self, in_channels=3, **kwargs):\n","        super(Yolov1, self).__init__()\n","        self.architecture = architecture_config\n","        self.in_channels = in_channels\n","        self.darknet = self._create_conv_layers(self.architecture)\n","        self.fcs = self._create_fcs(**kwargs)\n","\n","    def forward(self, x):\n","        x = self.darknet(x)\n","        return self.fcs(torch.flatten(x, start_dim=1))\n","\n","    def _create_conv_layers(self, architecture):\n","        \"\"\"\n","        Create convolutional layers based on the architecture configuration.\n","        \"\"\"\n","        layers = []\n","        in_channels = self.in_channels\n","\n","        for x in architecture:\n","            if type(x) == tuple:\n","                layers.append(\n","                    CNNBlock(\n","                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3]\n","                    )\n","                )\n","                in_channels = x[1]\n","\n","            elif type(x) == str:\n","                layers.append(nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n","\n","            elif type(x) == list:\n","                conv1, conv2, num_repeats = x\n","                for _ in range(num_repeats):\n","                    layers.append(\n","                        CNNBlock(\n","                            in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3]\n","                        )\n","                    )\n","                    layers.append(\n","                        CNNBlock(\n","                            conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3]\n","                        )\n","                    )\n","                    in_channels = conv2[1]\n","\n","        return nn.Sequential(*layers)\n","\n","    def _create_fcs(self, split_size, num_boxes, num_classes):\n","        \"\"\"\n","        Create fully connected layers for bounding box prediction.\n","\n","        Parameters:\n","            split_size: Grid size (e.g., 7x7 for YOLOv1).\n","            num_boxes: Number of bounding boxes per grid cell.\n","            num_classes: Number of object classes.\n","        \"\"\"\n","        S, B, C = split_size, num_boxes, num_classes\n","        return nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(1024 * S * S, 4096),\n","            nn.Dropout(0.0),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(4096, S * S * (C + B * 5)),  # Output layer\n","        )"],"metadata":{"id":"v9Hws9I7osho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class YoloLoss(nn.Module):\n","    \"\"\"\n","    Calculate the YOLO (v1) loss, combining components for box coordinates, object confidence,\n","    no-object confidence, and class probabilities.\n","    \"\"\"\n","\n","    def __init__(self, S=7, B=2, C=20):\n","        \"\"\"\n","        Initialize the YOLO loss.\n","\n","        Parameters:\n","            S (int): Grid size (e.g., 7 for 7x7 grid).\n","            B (int): Number of bounding boxes per grid cell.\n","            C (int): Number of classes.\n","        \"\"\"\n","        super(YoloLoss, self).__init__()\n","        self.mse = nn.MSELoss(reduction=\"sum\")\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","        self.lambda_noobj = 0.5  # Weight for no-object loss\n","        self.lambda_coord = 5  # Weight for box coordinate loss\n","\n","    def forward(self, predictions, target):\n","        \"\"\"\n","        Compute the YOLO loss.\n","\n","        Parameters:\n","            predictions (tensor): Predicted outputs (BATCH_SIZE, S * S * (C + B * 5)).\n","            target (tensor): Ground truth tensor of the same shape as predictions.\n","\n","        Returns:\n","            loss (tensor): Computed YOLO loss.\n","        \"\"\"\n","        # Reshape predictions to (BATCH_SIZE, S, S, C + B * 5)\n","        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n","\n","        # Extract IoU for both bounding boxes\n","        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n","        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n","        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n","\n","        # Determine which box has the highest IoU\n","        iou_maxes, bestbox = torch.max(ious, dim=0)\n","        exists_box = target[..., 20].unsqueeze(3)  # Iobj_i in the paper\n","\n","        # ======================== #\n","        #   FOR BOX COORDINATES    #\n","        # ======================== #\n","        box_predictions = exists_box * (\n","            bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25]\n","        )\n","        box_targets = exists_box * target[..., 21:25]\n","\n","        # Take the square root of width and height to ensure positive values\n","        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n","            torch.abs(box_predictions[..., 2:4] + 1e-6)\n","        )\n","        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n","\n","        # Compute box loss\n","        box_loss = self.mse(\n","            torch.flatten(box_predictions, end_dim=-2),\n","            torch.flatten(box_targets, end_dim=-2),\n","        )\n","\n","        # ==================== #\n","        #    FOR OBJECT LOSS   #\n","        # ==================== #\n","        pred_box = (\n","            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n","        )\n","        object_loss = self.mse(\n","            torch.flatten(exists_box * pred_box),\n","            torch.flatten(exists_box * target[..., 20:21]),\n","        )\n","\n","        # ======================= #\n","        #  FOR NO OBJECT LOSS     #\n","        # ======================= #\n","        no_object_loss = self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n","        )\n","        no_object_loss += self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n","        )\n","\n","        # ================== #\n","        #   FOR CLASS LOSS   #\n","        # ================== #\n","        class_loss = self.mse(\n","            torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n","            torch.flatten(exists_box * target[..., :20], end_dim=-2),\n","        )\n","\n","        # ================== #\n","        #   FINAL LOSS       #\n","        # ================== #\n","        loss = (\n","            self.lambda_coord * box_loss  # Box coordinates loss\n","            + object_loss  # Object confidence loss\n","            + self.lambda_noobj * no_object_loss  # No-object confidence loss\n","            + class_loss  # Class probability loss\n","        )\n","\n","        return loss"],"metadata":{"id":"aygIHtdboxmw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Setting and Training YOLOv1"],"metadata":{"id":"jCXu28fRo1pT"}},{"cell_type":"code","source":["seed = 123\n","torch.manual_seed(seed)"],"metadata":{"id":"XSF4bUlNo5Xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters and config\n","LEARNING_RATE = 2e-5\n","DEVICE = \"cuda\"  # Use \"cuda\" for GPU or \"cpu\" for CPU\n","BATCH_SIZE = 16  # Originally 64 in the YOLO paper, reduced for smaller GPUs\n","EPOCHS = 300  # Number of training epochs\n","NUM_WORKERS = 2  # Number of worker processes for data loading\n","PIN_MEMORY = True  # Pin memory for faster GPU transfers\n","LOAD_MODEL = False  # Whether to load a pre-trained model\n","LOAD_MODEL_FILE = \"yolov1.pth.tar\"  # File name for the pre-trained model (if LOAD_MODEL=True)"],"metadata":{"id":"12yC_Omwo_RB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Defind Augment Data"],"metadata":{"id":"P2BHDBAXpFqB"}},{"cell_type":"code","source":["# Input image dimensions\n","WIDTH = 448\n","HEIGHT = 448\n","\n","def get_train_transforms():\n","    \"\"\"\n","    Get data augmentation transformations for training.\n","    \"\"\"\n","    return A.Compose(\n","        [\n","            A.OneOf(\n","                [\n","                    A.HueSaturationValue(\n","                        hue_shift_limit=0.2,\n","                        sat_shift_limit=0.2,\n","                        val_shift_limit=0.2,\n","                        p=0.9,\n","                    ),\n","                    A.RandomBrightnessContrast(\n","                        brightness_limit=0.2,\n","                        contrast_limit=0.2,\n","                        p=0.9,\n","                    ),\n","                ],\n","                p=0.9,\n","            ),\n","            A.ToGray(p=0.01),  # Randomly convert some images to grayscale\n","            A.HorizontalFlip(p=0.2),  # Random horizontal flip\n","            A.VerticalFlip(p=0.2),  # Random vertical flip\n","            A.Resize(height=HEIGHT, width=WIDTH, p=1.0),  # Resize images\n","            A.Cutout(\n","                num_holes=8,\n","                max_h_size=64,\n","                max_w_size=64,\n","                fill_value=0,\n","                p=0.5,\n","            ),  # Random cutout augmentation\n","            ToTensorV2(p=1.0),  # Convert to PyTorch tensors\n","        ],\n","        p=1.0,\n","        bbox_params=A.BboxParams(\n","            format=\"yolo\",\n","            min_area=0,\n","            min_visibility=0,\n","            label_fields=[\"labels\"],\n","        ),\n","    )"],"metadata":{"id":"fi519hO3pLBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def get_valid_transforms():\n","    \"\"\"\n","    Get data transformations for validation (no augmentation).\n","    \"\"\"\n","    return A.Compose(\n","        [\n","            A.Resize(height=HEIGHT, width=WIDTH, p=1.0),  # Resize images\n","            ToTensorV2(p=1.0),  # Convert to PyTorch tensors\n","        ],\n","        p=1.0,\n","        bbox_params=A.BboxParams(\n","            format=\"yolo\",\n","            min_area=0,\n","            min_visibility=0,\n","            label_fields=[\"labels\"],\n","        ),\n","    )"],"metadata":{"id":"5C9bcrQfpUXE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Defind Mapping Object Class"],"metadata":{"id":"eo0PJoMcpNZ4"}},{"cell_type":"code","source":["# Class mapping for VOC dataset\n","class_mapping = {\n","    \"aeroplane\": 0,\n","    \"bicycle\": 1,\n","    \"bird\": 2,\n","    \"boat\": 3,\n","    \"bottle\": 4,\n","    \"bus\": 5,\n","    \"car\": 6,\n","    \"cat\": 7,\n","    \"chair\": 8,\n","    \"cow\": 9,\n","    \"diningtable\": 10,\n","    \"dog\": 11,\n","    \"horse\": 12,\n","    \"motorbike\": 13,\n","    \"person\": 14,\n","    \"pottedplant\": 15,\n","    \"sheep\": 16,\n","    \"sofa\": 17,\n","    \"train\": 18,\n","    \"tvmonitor\": 19,\n","}"],"metadata":{"id":"1kftLrumpSGE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8. Training and Val Loop"],"metadata":{"id":"dyaeAiH_pYRQ"}},{"cell_type":"code","source":["def train_fn(train_loader, model, optimizer, loss_fn, epoch):\n","    \"\"\"\n","    Training loop for one epoch.\n","\n","    Parameters:\n","        train_loader (DataLoader): DataLoader for training data.\n","        model (nn.Module): YOLOv1 model.\n","        optimizer (torch.optim.Optimizer): Optimizer for training.\n","        loss_fn (nn.Module): Loss function (YoloLoss).\n","        epoch (int): Current epoch number.\n","\n","    Returns:\n","        avg_mAP (float): Average mean Average Precision (mAP) for the epoch.\n","    \"\"\"\n","    model.train()  # Set model to training mode\n","    mean_loss = []\n","    mean_mAP = []\n","\n","    total_batches = len(train_loader)\n","    display_interval = total_batches // 5  # Display progress 5 times per epoch\n","\n","    for batch_idx, (x, y) in enumerate(train_loader):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","        # Forward pass\n","        out = model(x)\n","\n","        # Compute loss\n","        loss = loss_fn(out, y)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute predictions and mAP\n","        pred_boxes, true_boxes = get_bboxes_training(\n","            out, y, iou_threshold=0.5, threshold=0.4\n","        )\n","        mAP = mean_average_precision(\n","            pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n","        )\n","\n","        # Track metrics\n","        mean_loss.append(loss.item())\n","        mean_mAP.append(mAP.item())\n","\n","        # Display progress\n","        if batch_idx % display_interval == 0 or batch_idx == total_batches - 1:\n","            print(\n","                f\"Epoch: {epoch:3} \\t Iter: {batch_idx:3}/{total_batches:3} \"\n","                f\"\\t Loss: {loss.item():.10f} \\t mAP: {mAP.item():.10f}\"\n","            )\n","\n","    # Compute average metrics\n","    avg_loss = sum(mean_loss) / len(mean_loss)\n","    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n","\n","    print(colored(f\"Train \\t Loss: {avg_loss:.10f} \\t mAP: {avg_mAP:.10f}\", \"green\"))\n","\n","    return avg_mAP"],"metadata":{"id":"3W_wBGVkpaZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_fn(test_loader, model, loss_fn, epoch):\n","    \"\"\"\n","    Testing loop for one epoch.\n","\n","    Parameters:\n","        test_loader (DataLoader): DataLoader for test/validation data.\n","        model (nn.Module): YOLOv1 model.\n","        loss_fn (nn.Module): Loss function (YoloLoss).\n","        epoch (int): Current epoch number.\n","\n","    Returns:\n","        avg_mAP (float): Average mean Average Precision (mAP) for the epoch.\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode\n","    mean_loss = []\n","    mean_mAP = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (x, y) in enumerate(test_loader):\n","            x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","            # Forward pass\n","            out = model(x)\n","\n","            # Compute loss\n","            loss = loss_fn(out, y)\n","\n","            # Compute predictions and mAP\n","            pred_boxes, true_boxes = get_bboxes_training(\n","                out, y, iou_threshold=0.5, threshold=0.4\n","            )\n","            mAP = mean_average_precision(\n","                pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n","            )\n","\n","            # Track metrics\n","            mean_loss.append(loss.item())\n","            mean_mAP.append(mAP.item())\n","\n","    # Compute average metrics\n","    avg_loss = sum(mean_loss) / len(mean_loss)\n","    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n","\n","    print(colored(f\"Test \\t Loss: {avg_loss:.10f} \\t mAP: {avg_mAP:.10f}\", \"yellow\"))\n","\n","    return avg_mAP"],"metadata":{"id":"QvCsbWrjpc-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train () :\n","    # Initialize model , optimizer , loss\n","    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    loss_fn = YoloLoss()\n","\n","    # Load checkpoint\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","    # Create the full dataset\n","    train_dataset = CustomVOCDataset(\n","        root='./data',\n","        year='2012',\n","        image_set='train',\n","        download=True,\n","    )\n","\n","    train_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_train_transforms())\n","    testval_dataset = CustomVOCDataset(\n","        root='./data',\n","        year='2012',\n","        image_set='val',\n","        download=True,\n","    )\n","\n","    testval_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_val_transforms())\n","    custom_transforms = get_val_transforms()\n","\n","    #Split dataset into train, val, test sets using indices\n","    dataset_size = len(testval_dataset)\n","    val_size = int(0.15 * dataset_size)\n","    test_size = dataset_size - val_size\n","\n","    val_indices = list(range(val_size))\n","    test_indices = list(range(val_size, val_size + test_size))\n","\n","    # Create SubsetRandomSamplers\n","    val_sampler = SubsetRandomSampler(val_indices)\n","    test_sampler = SubsetRandomSampler(test_indices)\n","\n","    # Create DataLoaders using the samplers\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        drop_last=True)\n","\n","    val_loader = DataLoader(\n","        dataset=testval_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        sampler=val_sampler,\n","        drop_last=False)\n","\n","    test_loader = DataLoader(\n","        dataset=testval_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        sampler=test_sampler,\n","        drop_last=False)\n","\n","    best_mAP_train = 0\n","    best_mAP_test = 0\n","    best_mAP_val = 0\n","\n","    # Training loop\n","    for epoch in range(EPOCHS):\n","        train_mAP = train_fn(train_loader, model, optimizer, loss_fn, epoch)\n","        val_mAP = val_test_fn(val_loader, model, loss_fn, epoch)\n","        # Pass is_test = True for test set\n","        test_mAP = val_test_fn(test_loader, model, loss_fn, epoch, is_test=True)\n","\n","        # Update best mAP values\n","        if train_mAP > best_mAP_train:\n","            best_mAP_train = train_mAP\n","        if val_mAP > best_mAP_val:\n","            best_mAP_val = val_mAP\n","            #Save checkpoint when val mAP improves\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","            }\n","            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n","\n","        if test_mAP > best_mAP_test:\n","            best_mAP_test = test_mAP\n","\n","    print(colored(f'Best Train mAP: {best_mAP_train:3.10f}', 'green'))\n","    print(colored(f'Best Val mAP: {best_mAP_val:3.10f}', 'yellow'))\n","    print(colored(f'Best Test mAP: {best_mAP_test:3.10f}', 'red'))"],"metadata":{"id":"ISOcvjmzp2n4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_image_with_labels(image, ground_truth_boxes, predicted_boxes, class_mapping):\n","    \"\"\"\n","    Draw ground truth and predicted bounding boxes on an image with labels.\n","\n","    Parameters:\n","        image (numpy.ndarray): The input image.\n","        ground_truth_boxes (list): Ground truth bounding boxes, each formatted as [class_id, x_center, y_center, width, height].\n","        predicted_boxes (list): Predicted bounding boxes, similar format to ground_truth_boxes.\n","        class_mapping (dict): Mapping of class IDs to class names.\n","    \"\"\"\n","    # Invert the class mapping for quick lookup\n","    inverted_class_mapping = {v: k for k, v in class_mapping.items()}\n","\n","    # Convert the image to a numpy array and get its dimensions\n","    im = np.array(image)\n","    height, width, _ = im.shape\n","\n","    # Set up the plot\n","    fig, ax = plt.subplots(1)\n","    ax.imshow(im)\n","\n","    # Draw ground truth boxes (green)\n","    for box in ground_truth_boxes:\n","        label_index, box = box[0], box[1:]\n","        upper_left_x = box[0] - box[2] / 2\n","        upper_left_y = box[1] - box[3] / 2\n","        rect = patches.Rectangle(\n","            (upper_left_x * width, upper_left_y * height),\n","            box[2] * width,\n","            box[3] * height,\n","            linewidth=1,\n","            edgecolor=\"green\",\n","            facecolor=\"none\",\n","        )\n","        ax.add_patch(rect)\n","        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n","        ax.text(\n","            upper_left_x * width,\n","            upper_left_y * height,\n","            class_name,\n","            color=\"white\",\n","            fontsize=12,\n","            bbox=dict(facecolor=\"green\", alpha=0.5),\n","        )\n","\n","    # Draw predicted boxes (red)\n","    for box in predicted_boxes:\n","        label_index, box = box[0], box[1:]\n","        upper_left_x = box[0] - box[2] / 2\n","        upper_left_y = box[1] - box[3] / 2\n","        rect = patches.Rectangle(\n","            (upper_left_x * width, upper_left_y * height),\n","            box[2] * width,\n","            box[3] * height,\n","            linewidth=1,\n","            edgecolor=\"red\",\n","            facecolor=\"none\",\n","        )\n","        ax.add_patch(rect)\n","        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n","        ax.text(\n","            upper_left_x * width,\n","            upper_left_y * height,\n","            class_name,\n","            color=\"white\",\n","            fontsize=12,\n","            bbox=dict(facecolor=\"red\", alpha=0.5),\n","        )\n","\n","    plt.show()"],"metadata":{"id":"ihjOfNlzsua5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test():\n","    \"\"\"\n","    Test the YOLOv1 model on the validation dataset and visualize results.\n","    \"\"\"\n","    # Initialize the YOLO model\n","    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n","\n","    # Load the model's weights if specified\n","    if LOAD_MODEL:\n","        model.load_state_dict(torch.load(LOAD_MODEL_FILE)[\"state_dict\"])\n","\n","    # Prepare the test dataset and DataLoader\n","    test_dataset = CustomVOCDataset(\n","        root=\"./data\", image_set=\"val\", download=False\n","    )\n","    test_dataset.init_config_yolo(\n","        class_mapping=class_mapping,\n","        custom_transforms=get_valid_transforms(),\n","    )\n","    test_loader = DataLoader(\n","        dataset=test_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=False,\n","        drop_last=False,\n","    )\n","\n","    model.eval()\n","\n","    # Iterate over the test dataset\n","    for x, y in test_loader:\n","        x = x.to(DEVICE)\n","        out = model(x)\n","\n","        # Convert predictions and ground truth to bounding boxes\n","        pred_bboxes = cellboxes_to_boxes(out)\n","        gt_bboxes = cellboxes_to_boxes(y)\n","\n","        # Visualize the first 8 images with their bounding boxes\n","        for idx in range(8):\n","            pred_box = non_max_suppression(\n","                pred_bboxes[idx],\n","                iou_threshold=0.5,\n","                threshold=0.4,\n","                box_format=\"midpoint\",\n","            )\n","            gt_box = non_max_suppression(\n","                gt_bboxes[idx],\n","                iou_threshold=0.5,\n","                threshold=0.4,\n","                box_format=\"midpoint\",\n","            )\n","            image = x[idx].permute(1, 2, 0).cpu().numpy() / 255.0\n","            plot_image_with_labels(image, gt_box, pred_box, class_mapping)\n","        break  # Stop after processing the first batch"],"metadata":{"id":"jOOYp88nsx71"},"execution_count":null,"outputs":[]}]}